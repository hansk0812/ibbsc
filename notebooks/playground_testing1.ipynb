{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "import pickle\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import tqdm\n",
    "import data_utils\n",
    "from random import seed\n",
    "import info_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "seed(12)\n",
    "torch.manual_seed(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset path\n",
    "data_path = \"../data/var_u.mat\" # Orig IB data\n",
    "\n",
    "# Run on GPU if possible\n",
    "try_gpu = False\n",
    "if try_gpu:\n",
    "    cuda = torch.cuda.is_available() \n",
    "    device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(\"Using \"+ str(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_split = True\n",
    "if random_split:\n",
    "    X_train, X_test, y_train, y_test = data_utils.load_data(data_path, 819)\n",
    "else:\n",
    "    trn, tst = utils.get_ib_data()\n",
    "    X_train, y_train, c_train = utils.tensor_casting(trn)\n",
    "    X_test, y_test, c_test = utils.tensor_casting(tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### lr=0.0004 for full batch with Adam. \n",
    "# batch_size 256 lr = 0.01 adam lr_scheduler 300 patience, default params else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"loss_function\" : nn.CrossEntropyLoss(),\n",
    "    \"batch_size\" : 256,\n",
    "    \"epochs\" : 100,\n",
    "}\n",
    "\n",
    "layer_sizes = [12,10,7,5,4,3,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for pytorch\n",
    "if config[\"batch_size\"] != \"full\":\n",
    "    train_loader = data_utils.create_dataloader(X_train, y_train, config[\"batch_size\"])\n",
    "    test_loader = data_utils.create_dataloader(X_test, y_test, config[\"batch_size\"])\n",
    "else:\n",
    "    train_loader = data_utils.create_dataloader(X_train, y_train, len(X_train))\n",
    "    test_loader = data_utils.create_dataloader(X_test, y_test, len(X_test))\n",
    "\n",
    "# Activitiy loaders\n",
    "full_X, full_y = np.concatenate((X_train, X_test)), np.concatenate((y_train, y_test))\n",
    "act_full_loader = data_utils.create_dataloader(full_X, full_y, len(full_X))\n",
    "act_train_loader = data_utils.create_dataloader(X_train, y_train, len(X_train))\n",
    "act_test_loader = data_utils.create_dataloader(X_test, y_test, len(X_test))\n",
    "act_loaders = [act_full_loader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What do collect during training.\n",
    "# Gradients of weights for all layers: something like ib_model.h3.weight.grad\n",
    "# Weights for each layer: ib_model.h3.weight\n",
    "# Activity: Save it in the forward pass and keep track of batch things.\n",
    "\n",
    "# Compute L2 norm of weights\n",
    "# Mean of gradients\n",
    "# Std of gradients \n",
    "# Activity <- needed for MI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNN(nn.Module):\n",
    "    def __init__(self, layer_sizes, activation=\"tanh\", seed=0):\n",
    "        super(FNN, self).__init__()\n",
    "        torch.manual_seed(seed)\n",
    "        self.linears = nn.ModuleList()\n",
    "        self.activation = activation\n",
    "        self.num_layers = len(layer_sizes)\n",
    "        self.inp_size = layer_sizes[0]\n",
    "        \n",
    "        h_in = self.inp_size\n",
    "        for h_out in layer_sizes[1:]:\n",
    "            self.linears.append(nn.Linear(h_in, h_out))\n",
    "            h_in = h_out\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        activations = [] #TODO: Could be nicer\n",
    "        for idx in range(self.num_layers-2):\n",
    "            x = self.linears[idx](x)\n",
    "            if self.activation == \"tanh\":\n",
    "                x = torch.tanh(x)\n",
    "            elif self.activation == \"relu\":\n",
    "                x = F.relu(x)\n",
    "            else:\n",
    "                raise(\"Activation Function not supported. Please choose \\\"tanh\\\" or \\\"relu\\\"\")\n",
    "            if not self.training: #Internal flag in model\n",
    "                activations.append(x)\n",
    "        x = self.linears[-1](x)\n",
    "        if not self.training: #Internal flag in model\n",
    "            activations.append(F.softmax(x, dim=-1))\n",
    "            \n",
    "        return x, F.softmax(x, dim=-1), activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ib_model = FNN(layer_sizes, activation=\"tanh\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#512, lr 0.003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(ib_model.parameters(), lr=0.003, momentum=0.9z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(ib_model.parameters(), lr = 0.0004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distributions import truncated_normal_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "import pickle\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from distributions import truncated_normal_\n",
    "import tqdm\n",
    "import data_utils\n",
    "from random import seed\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, cfg, model, optimizer, device):\n",
    "        self.opt = optimizer\n",
    "        self.device = device\n",
    "        self.loss_function = cfg[\"loss_function\"]\n",
    "        self.epochs = cfg[\"epochs\"]\n",
    "        self.model = model\n",
    "        self.hidden_activations = [] # index 1: epoch num, index2 : layer_num\n",
    "        self.hidden_activations_test = []\n",
    "        self.hidden_activations_train = []\n",
    "        self.val_loss = []\n",
    "        self.train_loss = []\n",
    "        self.full_loss = []\n",
    "        self.error_train = []\n",
    "        self.error_test = []\n",
    "        #self.weights = dict() #  Not currently in use, but if plot of grad of weights are needed we need this\n",
    "        #self.ws_grads = dict() # Not currently in use, but if plot of grad of weights are needed we need this\n",
    "        \n",
    "    def _init_weights(self, layer):\n",
    "        \"\"\"\n",
    "        Initialize the weights and bias for each linear layer in the model.\n",
    "        \"\"\"\n",
    "        if type(layer) == nn.Linear:\n",
    "            # Truncated normal is only available in their unstable nightly version\n",
    "            #nn.init.trunc_normal_(layer.weight, mean=0, std=1/np.sqrt(layer.weight.shape[0]))\n",
    "            truncated_normal_(layer.weight)\n",
    "            if layer.bias != None: \n",
    "                layer.bias.data.fill_(0.00)\n",
    "\n",
    "        \n",
    "    def _get_epoch_activity(self, loader, val=False):\n",
    "        \"\"\"\n",
    "        After each epoch save the activation of each hidden layer\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        v_loss = 0\n",
    "        acc = 0\n",
    "        with torch.no_grad(): # Speeds up very little by turning autograd engine off.\n",
    "            for data, label in loader:\n",
    "                data, label= data.to(self.device), label.long().to(self.device)\n",
    "                yhat, yhat_softmax, activations = self.model(data)\n",
    "                v_loss += self.loss_function(yhat, label).item()\n",
    "                acc += self._get_number_correct(yhat_softmax, label).item()\n",
    "                \n",
    "        v_loss = v_loss / len(loader.dataset)\n",
    "        if val:\n",
    "            acc = acc / float(len(loader.dataset))\n",
    "            self.error_test.append(1-acc)\n",
    "            #print('Validation loss: {:.7f},  Validation Acc. {:.4f}'.format(v_loss, acc))\n",
    "            self.val_loss.append(v_loss)\n",
    "        else:\n",
    "           #print('Full loss: {:.7f}'.format(v_loss))\n",
    "            self.full_loss.append(v_loss)\n",
    "        return v_loss, list(map(lambda x:x.cpu().numpy(), activations))\n",
    "    \n",
    "    \n",
    "    def _loop_act_loaders(self, loaders):\n",
    "        \"\"\"\n",
    "        If we want to save the activity after each epoch for more that one dataset.\n",
    "        I.e some papers save activity for both train, test and test+train. \n",
    "        Note that the order is important here. TODO: change loaders to be a dict.\n",
    "        \"\"\"\n",
    "        counter = 0\n",
    "        for loader in loaders:\n",
    "            v_loss, act = self._get_epoch_activity(loader)\n",
    "            if counter == 0:\n",
    "                self.hidden_activations.append(act)\n",
    "            elif counter == 1:\n",
    "                self.hidden_activations_train.append(act)\n",
    "            elif counter == 2:\n",
    "                self.hidden_activations_test.append(act)\n",
    "            counter += 1\n",
    "            \n",
    "    \n",
    "    def _get_number_correct(self, output, target):\n",
    "        \"\"\"\n",
    "        Returns number of correct predictions from the softmax output. \n",
    "        Requires target to be a flat vector i.e not one-hot encoded.\n",
    "        \"\"\"\n",
    "        n_corr = (target == output.argmax(dim=1)).sum()\n",
    "        return n_corr \n",
    "    \n",
    "    \n",
    "    def train(self, train_loader, test_loader, act_loaders):\n",
    "        MI_tracker = 1\n",
    "        self.model.apply(self._init_weights) #Init kernel weights\n",
    "        #scheduler = optim.lr_scheduler.ReduceLROnPlateau(self.opt, 'min', verbose=True, patience=300)\n",
    "        for epoch in tqdm.tqdm(range(1, self.epochs+1)):\n",
    "            ### START MAIN TRAIN LOOP ###\n",
    "            self.model.train()\n",
    "            train_loss = 0\n",
    "            acc_train = 0\n",
    "            for train_data, label in train_loader: \n",
    "                train_data, label  = train_data.to(self.device), label.long().to(self.device)\n",
    "                yhat, yhat_softmax, _ = self.model(train_data)\n",
    "                loss = self.loss_function(yhat, label)\n",
    "                acc_train += self._get_number_correct(yhat_softmax, label).item()\n",
    "                self.opt.zero_grad()\n",
    "                loss.backward()\n",
    "                train_loss += loss.item()\n",
    "                self.opt.step()\n",
    "            self.error_train.append(1-acc_train)\n",
    "            train_loss = train_loss / len(train_loader.dataset)\n",
    "            self.train_loss.append(train_loss)\n",
    "            #print('Epoch: {} Train loss: {:.7f},  Train Acc. {:.4f}'.format(epoch, train_loss, acc_train / float(len(train_loader.dataset))))\n",
    "            ### STOP MAIN TRAIN LOOP ###\n",
    "        \n",
    "            ### RUN ON VALIDATION DATA ###\n",
    "            val_loss = self._get_epoch_activity(test_loader, val=True)[0]\n",
    "            #scheduler.step(val_loss) #Reduce LR on plateau.\n",
    "            \n",
    "            ### SAVE ACTIVATION ON FULL DATA ###\n",
    "            self._loop_act_loaders(act_loaders)\n",
    "            #print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = Trainer(config, ib_model, optimizer, device)\n",
    "tr.train(train_loader, test_loader, act_loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_loaders[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MI:\n",
    "    def __init__(self, activity, data_loader, act, num_of_bins):\n",
    "        self.activity = activity\n",
    "        self.num_of_bins = num_of_bins\n",
    "        self.act=act\n",
    "        \n",
    "        if act == \"tanh\" or act == \"elu\":\n",
    "            self.min_val = -1\n",
    "            self.max_val = 1\n",
    "        else:\n",
    "            self.min_val = 0\n",
    "            self.max_val = info_utils.get_max_value(activity)\n",
    "        \n",
    "        self.X = data_loader.dataset.tensors[0].numpy()\n",
    "        self.y_flat = data_loader.dataset.tensors[1].numpy()\n",
    "        \n",
    "        classes = len(np.unique(self.y_flat))\n",
    "        self.y_onehot = np.eye(classes)[self.y_flat.astype(\"int\")]\n",
    "        nb_classes = self.y_onehot[0]\n",
    "        self.y_idx_label = {x : None for x in nb_classes}\n",
    "        for i in self.y_idx_label:\n",
    "            self.y_idx_label[i] = i == self.y_flat\n",
    "\n",
    "\n",
    "    def entropy(self, bins, activations):\n",
    "        binned = np.digitize(activations, bins)\n",
    "        binned = bins[np.digitize(np.squeeze(activations.reshape(1, -1)), bins) - 1].reshape(len(activations), -1)\n",
    "        print(binned)\n",
    "        _, unique_layers = np.unique(binned, axis=0, return_counts=True)\n",
    "        prob_hidden_layers = unique_layers / sum(unique_layers)\n",
    "        return -np.sum(prob_hidden_layers * np.log2(prob_hidden_layers))\n",
    "\n",
    "    \n",
    "    def mi_binning(self, labelixs, activations_layer, bins):\n",
    "        entropy_layer = self.entropy(bins, activations_layer)\n",
    "        entropy_layer_output = 0\n",
    "        for label, ixs in labelixs.items():\n",
    "            h = self.entropy(bins, activations_layer[ixs,:])\n",
    "            entropy_layer_output += ixs.mean() * h\n",
    "        return entropy_layer, (entropy_layer - entropy_layer_output)\n",
    "\n",
    "    \n",
    "    def get_MI(self, method):\n",
    "        all_MI_XH = [] # Contains I(X;H) and stores it as (epoch_num, layer_num)\n",
    "        all_MI_YH = [] # Contains I(Y;H) and stores it as (epoch_num, layer_num\n",
    "        if method == \"fixed\":\n",
    "            bins = np.linspace(self.min_val, self.max_val, self.num_of_bins)\n",
    "        elif method == \"adaptive\":\n",
    "            max_vals, adapt_bins = info_utils.get_bins_layers(self.activity, self.num_of_bins, self.act)\n",
    "        else:\n",
    "            raise(\"Method not supported. Pick fixed or adaptive\")\n",
    "\n",
    "        for idx, epoch in tqdm.tqdm(enumerate(self.activity)):\n",
    "            temp_MI_XH = []\n",
    "            temp_MI_YH = []\n",
    "            for layer_num in range(len(epoch)):\n",
    "                if method == \"fixed\":\n",
    "                    MI_XH, MI_YH = self.mi_binning(self.y_idx_label,epoch[layer_num], bins)\n",
    "                elif method == \"adaptive\":\n",
    "                    MI_XH, MI_YH = self.mi_binning(self.y_idx_label,epoch[layer_num], adapt_bins[idx][layer_num])\n",
    "                temp_MI_XH.append(MI_XH)\n",
    "                temp_MI_YH.append(MI_YH)\n",
    "            all_MI_XH.append(temp_MI_XH)\n",
    "            all_MI_YH.append(temp_MI_YH)\n",
    "\n",
    "        return all_MI_XH, all_MI_YH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutual_inf = MI(tr.hidden_activations, act_full_loader,act=\"tanh\", num_of_bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutual_inf.max_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MI_XH, MI_YH = mutual_inf.get_MI(method=\"adaptive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MI_XH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.98712242 0.00959309]\n",
    " [0.98712242 0.00959309]\n",
    " [0.89632577 0.07310463]]\n",
    "[[0.49675086 0.39470944]\n",
    " [0.49675086 0.39470944]\n",
    " [0.60034305 0.29429334]\n",
    " ...\n",
    " [0.49675086 0.39470944]\n",
    " [0.05872701 0.92584896]\n",
    " [0.9596023  0.03073659]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for i in tqdm.tqdm(range(20)):\n",
    "    ib_model = FNN(layer_sizes, activation=\"relu\").to(device)\n",
    "    optimizer = optim.Adam(ib_model.parameters(), lr=0.0004)\n",
    "    tr = Trainer(config, ib_model, optimizer, device)\n",
    "    tr.train(train_loader, test_loader, act_loaders)\n",
    "    mutual_inf = MI(tr.hidden_activations, act_full_loader,act=\"relu\", num_of_bins=100)\n",
    "    MI_XH, MI_YH = mutual_inf.get_MI()\n",
    "    with open('MI_XH_MI_YH_run_{}_512_relu.pickle'.format(i), 'wb') as f:\n",
    "        pickle.dump([MI_XH, MI_YH], f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 10, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "\n",
    "def plot_layer_MI(MI, y_label, subplot=False, dataset=\"Full IB Data\"):\n",
    "    if subplot:\n",
    "        subplots_num = MI.shape[1]\n",
    "        fig, axs = plt.subplots(1, subplots_num, figsize=(15,5))\n",
    "\n",
    "        for idx in range(MI.shape[1]):\n",
    "            axs[idx].plot(MI[:,idx], label=\"Hidden Layer {}\".format(idx+1))\n",
    "            axs[idx].set_ylabel(y_label)\n",
    "            axs[idx].set_xlabel('Epoch')\n",
    "            axs[idx].legend()\n",
    "\n",
    "        fig.show()\n",
    "        fig.savefig('test.png')\n",
    "        return\n",
    "\n",
    "    fig, ax = plt.subplots(1)\n",
    "    for idx in range(MI.shape[1]):\n",
    "            ax.plot(MI[:,idx], label=\"Hidden Layer {}\".format(idx+1))\n",
    "            ax.set_ylabel(y_label)\n",
    "            ax.set_xlabel('Epoch')\n",
    "            ax.legend()\n",
    "    fig.show()\n",
    "    fig.savefig('test.png')\n",
    "    return \n",
    "\n",
    "\n",
    "def plot_info_plan(MI_XH, MI_YH, cbar_epochs=\"8000\", dataset=\"Full IB Data\"):\n",
    "    fig, ax = plt.subplots(1)\n",
    "    \n",
    "    # Similar to ibsgd and idnn\n",
    "    # source https://stackoverflow.com/questions/8389636/creating-over-20-unique-legend-colors-using-matplotlib\n",
    "    cmap = plt.get_cmap('gnuplot')\n",
    "    norm_colors  = matplotlib.colors.Normalize(vmin=0, vmax=MI_XH.shape[0]-1)\n",
    "    mapcs = matplotlib.cm.ScalarMappable(norm=norm_colors, cmap=cmap)\n",
    "    colors = [mapcs.to_rgba(i) for i in range(MI_XH.shape[0])]\n",
    "    ax.set_prop_cycle(color = colors)\n",
    "\n",
    "    for j in tqdm.tqdm(range(MI_XH.shape[0])):\n",
    "        ax.scatter(MI_XH[j, :], MI_YH[j, :], s=20, alpha=0.9, zorder=3)\n",
    "        ax.plot(MI_XH[j, :], MI_YH[j, :], alpha=0.01, zorder=1)\n",
    "\n",
    "    ax.set_title('Information Plane - {}'.format(dataset))\n",
    "    ax.set_xlabel('I(X;T)')\n",
    "    ax.set_ylabel('I(T;Y)')\n",
    "    cbar = fig.colorbar(mapcs, ticks=[])\n",
    "    cbar.set_label(\"Epochs\")\n",
    "    #https://stackoverflow.com/questions/28808143/putting-tick-values-on-top-and-bottom-of-matplotlib-colorbar\n",
    "    cbar.ax.text(0.5, -0.01, '0', transform=cbar.ax.transAxes, va='top', ha='center')\n",
    "    cbar.ax.text(0.5, 1.0, cbar_epochs, transform=cbar.ax.transAxes, va='bottom', ha='center')\n",
    "    fig.savefig(f'IP_FULLDATA.png')\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layer_MI(np.array(MI_XH[:]), \"I(X;H)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layer_MI(np.array(MI_YH[:]), \"I(Y;H)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_info_plan(np.array(MI_XH[:]), np.array(MI_YH[:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tr.train_loss)\n",
    "plt.plot(tr.val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutual_inf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_unique_probs(x):\n",
    "    uniqueids = np.ascontiguousarray(x).view(np.dtype((np.void, x.dtype.itemsize * x.shape[1])))\n",
    "    _, unique_inverse, unique_counts = np.unique(uniqueids, return_index=False, return_inverse=True, return_counts=True)\n",
    "    return np.asarray(unique_counts / float(sum(unique_counts))), unique_inverse\n",
    "\n",
    "def bin_calc_information(inputdata, layerdata, num_of_bins):\n",
    "    p_xs, unique_inverse_x = get_unique_probs(inputdata)\n",
    "    \n",
    "    bins = np.linspace(-1, 1, num_of_bins, dtype='float32') \n",
    "    digitized = bins[np.digitize(np.squeeze(layerdata.reshape(1, -1)), bins) - 1].reshape(len(layerdata), -1)\n",
    "    p_ts, _ = get_unique_probs( digitized )\n",
    "    #print(p_ts)\n",
    "    H_LAYER = -np.sum(p_ts * np.log2(p_ts))\n",
    "    #print(H_LAYER)\n",
    "    H_LAYER_GIVEN_INPUT = 0.\n",
    "    print(unique_inverse_x)\n",
    "    for xval in np.arange(len(p_xs)):\n",
    "        p_t_given_x, _ = get_unique_probs(digitized[unique_inverse_x == xval, :])\n",
    "        #print(p_t_given_x)\n",
    "        H_LAYER_GIVEN_INPUT += - p_xs[xval] * np.sum(p_t_given_x * np.log2(p_t_given_x))\n",
    "    #print(H_LAYER_GIVEN_INPUT)\n",
    "    return H_LAYER - H_LAYER_GIVEN_INPUT\n",
    "\n",
    "def bin_calc_information2(labelixs, layerdata, binsize):\n",
    "    # This is even further simplified, where we use np.floor instead of digitize\n",
    "    def get_h(d):\n",
    "        digitized = np.floor(d / binsize).astype('int')\n",
    "        p_ts, _ = get_unique_probs( digitized )\n",
    "        return -np.sum(p_ts * np.log2(p_ts))\n",
    "\n",
    "    H_LAYER = get_h(layerdata)\n",
    "    H_LAYER_GIVEN_OUTPUT = 0\n",
    "    for label, ixs in labelixs.items():\n",
    "        H_LAYER_GIVEN_OUTPUT += ixs.mean() * get_h(layerdata[ixs,:])\n",
    "    return H_LAYER, H_LAYER - H_LAYER_GIVEN_OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "# Original MI computation code from https://github.com/ravidziv/IDNNs\n",
    "\n",
    "def extract_probs(label, x):\n",
    "    \"\"\"calculate the probabilities of the given data and labels p(x), p(y) and (y|x)\"\"\"\n",
    "    pys = np.sum(label, axis=0) / float(label.shape[0])\n",
    "    b = np.ascontiguousarray(x).view(np.dtype((np.void, x.dtype.itemsize * x.shape[1])))\n",
    "    unique_array, unique_indices, unique_inverse_x, unique_counts = \\\n",
    "        np.unique(b, return_index=True, return_inverse=True, return_counts=True)\n",
    "    unique_a = x[unique_indices]\n",
    "    b1 = np.ascontiguousarray(unique_a).view(np.dtype((np.void, unique_a.dtype.itemsize * unique_a.shape[1])))\n",
    "    pxs = unique_counts / float(np.sum(unique_counts))\n",
    "    p_y_given_x = []\n",
    "    for i in range(0, len(unique_array)):\n",
    "        indexs = unique_inverse_x == i\n",
    "        py_x_current = np.mean(label[indexs, :], axis=0)\n",
    "        p_y_given_x.append(py_x_current)\n",
    "    p_y_given_x = np.array(p_y_given_x).T\n",
    "    b_y = np.ascontiguousarray(label).view(np.dtype((np.void, label.dtype.itemsize * label.shape[1])))\n",
    "    unique_array_y, unique_indices_y, unique_inverse_y, unique_counts_y = \\\n",
    "        np.unique(b_y, return_index=True, return_inverse=True, return_counts=True)\n",
    "    pys1 = unique_counts_y / float(np.sum(unique_counts_y))\n",
    "    return pys, pys1, p_y_given_x, b, unique_a, unique_inverse_x, unique_inverse_y, pxs\n",
    "\n",
    "def calc_information_sampling(data, bins, pys1, pxs, label, b, p_YgX, unique_inverse_x,  unique_inverse_y):\n",
    "    bins = bins.astype(np.float32)\n",
    "    num_of_bins = bins.shape[0]\n",
    "    # bins = stats.mstats.mquantiles(np.squeeze(data.reshape(1, -1)), np.linspace(0,1, num=num_of_bins))\n",
    "    # hist, bin_edges = np.histogram(np.squeeze(data.reshape(1, -1)), normed=True)\n",
    "    digitized = bins[np.digitize(np.squeeze(data.reshape(1, -1)), bins) - 1].reshape(len(data), -1)\n",
    "    b2 = np.ascontiguousarray(digitized).view(\n",
    "        np.dtype((np.void, digitized.dtype.itemsize * digitized.shape[1])))\n",
    "    unique_array, unique_inverse_t, unique_counts = \\\n",
    "        np.unique(b2, return_index=False, return_inverse=True, return_counts=True)\n",
    "    p_ts = unique_counts / float(sum(unique_counts))\n",
    "    PXs, PYs = np.asarray(pxs).T, np.asarray(pys1).T\n",
    "    local_IXT, local_ITY = calc_information_from_mat(PXs, PYs, p_ts, digitized, unique_inverse_x, unique_inverse_y,\n",
    "                                                     unique_array)\n",
    "    return local_IXT, local_ITY\n",
    "\n",
    "def calc_condtion_entropy(px, t_data, unique_inverse_x):\n",
    "    # Condition entropy of t given x\n",
    "    H2X_array = np.array(list(\n",
    "        calc_entropy_for_specipic_t(t_data[unique_inverse_x == i, :], px[i])\n",
    "                                   for i in range(px.shape[0])))\n",
    "    H2X = np.sum(H2X_array)\n",
    "    return H2X\n",
    "\n",
    "\n",
    "def calc_information_from_mat(px, py, ps2, data, unique_inverse_x, unique_inverse_y, unique_array):\n",
    "    \"\"\"Calculate the MI based on binning of the data\"\"\"\n",
    "    H2 = -np.sum(ps2 * np.log2(ps2))\n",
    "    H2X = calc_condtion_entropy(px, data, unique_inverse_x)\n",
    "    H2Y = calc_condtion_entropy(py.T, data, unique_inverse_y)\n",
    "    IY = H2 - H2Y\n",
    "    IX = H2 - H2X\n",
    "    return IX, IY\n",
    "\n",
    "def calc_entropy_for_specipic_t(current_ts, px_i):\n",
    "    \"\"\"Calc entropy for specipic t\"\"\"\n",
    "    b2 = np.ascontiguousarray(current_ts).view(\n",
    "        np.dtype((np.void, current_ts.dtype.itemsize * current_ts.shape[1])))\n",
    "    unique_array, unique_inverse_t, unique_counts = \\\n",
    "        np.unique(b2, return_index=False, return_inverse=True, return_counts=True)\n",
    "    p_current_ts = unique_counts / float(sum(unique_counts))\n",
    "    p_current_ts = np.asarray(p_current_ts, dtype=np.float64).T\n",
    "    H2X = px_i * (-np.sum(p_current_ts * np.log2(p_current_ts)))\n",
    "    return H2X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pys, pys1, p_YgX, b, unique_a, unique_inverse_x, unique_inverse_y, pxs = extract_probs(mutual_inf.y_onehot, mutual_inf.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_num_of_bins = 30\n",
    "bins = np.linspace(-1, 1, test_num_of_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_bins = [-1.0001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = tr.hidden_activations[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique = np.unique(np.squeeze(layer.reshape(1,-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.setdiff1d(unique,layer_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "borders=sorted(np.setdiff1d(unique,layer_bins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "borders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_IXT, local_ITY = calc_information_sampling(np.array(tr.hidden_activations[9][3]), bins, pys1, pxs, mutual_inf.y_onehot, b, p_YgX, unique_inverse_x, unique_inverse_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_IXT, local_ITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_calc_information2(mutual_inf.y_idx_label, tr.hidden_activations[9][3], binsize=0.07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_calc_information(mutual_inf.X, tr.hidden_activations[0][1], test_num_of_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutual_inf.mi_binning(mutual_inf.y_idx_label, tr.hidden_activations[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [0.03344727, 0.00048828, 0.07592773, 0.09521484, 0.1940918,  0.21777344,\n",
    " 0.00585938, 0.01611328, 0.05102539, 0.03442383, 0.15820312, 0.11743164]\n",
    "[0.03344727 0.00048828 0.07592773 0.09521484 0.1940918  0.21777344\n",
    " 0.00585938 0.01611328 0.05102539 0.03442383 0.15820312 0.11743164]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [0.03344727, 0.21777344, 0.1940918,  0.11743164, 0.15820312, 0.07592773,\n",
    " 0.09521484, 0.03442383, 0.05102539, 0.00585938, 0.01611328, 0.00048828]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "NUM_CORES = multiprocessing.cpu_count()\n",
    "\n",
    "\n",
    "def calc_entropy_for_specipic_t(current_ts, px_i):\n",
    "    \"\"\"Calc entropy for specipic t\"\"\"\n",
    "    #print(current_ts.shape)\n",
    "    b2 = np.ascontiguousarray(current_ts).view(\n",
    "        np.dtype((np.void, current_ts.dtype.itemsize * current_ts.shape[1])))\n",
    "    unique_array, _, unique_counts = \\\n",
    "        np.unique(b2, return_index=False, return_inverse=True, return_counts=True)\n",
    "    p_current_ts = unique_counts / float(sum(unique_counts))\n",
    "    p_current_ts = np.asarray(p_current_ts, dtype=np.float64).T\n",
    "    H2X = px_i * (-np.sum(p_current_ts * np.log2(p_current_ts)))\n",
    "    return H2X\n",
    "\n",
    "\n",
    "def calc_condtion_entropy(px, t_data, unique_inverse_x):\n",
    "    # Condition entropy of t given x\n",
    "    H2X_array = np.array(\n",
    "        Parallel(n_jobs=NUM_CORES)(delayed(calc_entropy_for_specipic_t)(t_data[unique_inverse_x == i, :], px[i])\n",
    "                                   for i in range(px.shape[0])))\n",
    "    H2X = np.sum(H2X_array)\n",
    "    return H2X\n",
    "\n",
    "\n",
    "def calc_information_from_mat(px, py, ps2, data, unique_inverse_x, unique_inverse_y, unique_array):\n",
    "    \"\"\"Calculate the MI based on binning of the data\"\"\"\n",
    "    H2 = -np.sum(ps2 * np.log2(ps2))\n",
    "    H2X = calc_condtion_entropy(px, data, unique_inverse_x)\n",
    "    H2Y = calc_condtion_entropy(py.T, data, unique_inverse_y)\n",
    "    IY = H2 - H2Y\n",
    "    IX = H2 - H2X\n",
    "    return IX, IY\n",
    "\n",
    "\n",
    "def calc_probs(t_index, unique_inverse, label, b, b1, len_unique_a):\n",
    "    \"\"\"Calculate the p(x|T) and p(y|T)\"\"\"\n",
    "    indexs = unique_inverse == t_index\n",
    "    p_y_ts = np.sum(label[indexs], axis=0) / label[indexs].shape[0]\n",
    "    unique_array_internal, unique_counts_internal = \\\n",
    "        np.unique(b[indexs], return_index=False, return_inverse=False, return_counts=True)\n",
    "    indexes_x = np.where(np.in1d(b1, b[indexs]))\n",
    "    p_x_ts = np.zeros(len_unique_a)\n",
    "    p_x_ts[indexes_x] = unique_counts_internal / float(sum(unique_counts_internal))\n",
    "    return p_x_ts, p_y_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_information_sampling(data, bins, pys1, pxs, label, b, b1, len_unique_a, p_YgX, unique_inverse_x,\n",
    "                              unique_inverse_y):\n",
    "    bins = bins.astype(np.float32)\n",
    "    #num_of_bins = bins.shape[0]\n",
    "    # bins = stats.mstats.mquantiles(np.squeeze(data.reshape(1, -1)), np.linspace(0,1, num=num_of_bins))\n",
    "    # hist, bin_edges = np.histogram(np.squeeze(data.reshape(1, -1)), normed=True)\n",
    "    digitized = bins[np.digitize(np.squeeze(data.reshape(1, -1)), bins) - 1].reshape(len(data), -1)\n",
    "    b2 = np.ascontiguousarray(digitized).view(\n",
    "        np.dtype((np.void, digitized.dtype.itemsize * digitized.shape[1])))\n",
    "    unique_array, unique_inverse_t, unique_counts = \\\n",
    "        np.unique(b2, return_index=False, return_inverse=True, return_counts=True)\n",
    "    p_ts = unique_counts / float(sum(unique_counts))\n",
    "    PXs, PYs = np.asarray(pxs).T, np.asarray(pys1).T\n",
    "    local_IXT, local_ITY = calc_information_from_mat(PXs, PYs, p_ts, digitized, unique_inverse_x, unique_inverse_y,unique_array)\n",
    "    return local_IXT, local_ITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_information_for_layer_with_other(data, bins, unique_inverse_x, unique_inverse_y, label,\n",
    "                                          b, b1, len_unique_a, pxs, p_YgX, pys1,layer,num_of_bins):\n",
    "    local_IXT, local_ITY = calc_information_sampling(data, bins, pys1, pxs, label, b, b1,\n",
    "                             len_unique_a, p_YgX, unique_inverse_x,unique_inverse_y)\n",
    "    params = {}\n",
    "    params['local_IXT'] = local_IXT\n",
    "    params['local_ITY'] = local_ITY\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_information_for_epoch(iter_index, interval_information_display,ws_iter_index, bins, unique_inverse_x,\n",
    "                               unique_inverse_y, label, b, b1,\n",
    "                               len_unique_a, pys, pxs, py_x, pys1, model_path,\n",
    "                               input_size, layerSize, num_of_bins,per_layer_bins=False,\n",
    "                               num_of_samples=100, sigma=0.5, ss=[], ks=[]):\n",
    "    \"\"\"Calculate the information for all the layers for specific epoch\"\"\"\n",
    "    np.random.seed(None)\n",
    "    if per_layer_bins:\n",
    "        params = np.array(\n",
    "\t\t[calc_information_for_layer_with_other(data=ws_iter_index[i], bins=bins[i], unique_inverse_x=unique_inverse_x,\n",
    "                                unique_inverse_y=unique_inverse_y, label=label,\n",
    "                                 b=b, b1=b1, len_unique_a=len_unique_a, pxs=pxs,\n",
    "                                 p_YgX=py_x, pys1=pys1,layer=i,num_of_bins=num_of_bins)\n",
    "            for i in range(len(ws_iter_index))])\n",
    "    else:\n",
    "        params = np.array(\n",
    "\t\t[calc_information_for_layer_with_other(data=ws_iter_index[i], bins=bins, unique_inverse_x=unique_inverse_x,\n",
    "\t\t\t                    unique_inverse_y=unique_inverse_y, label=label,\n",
    "\t                             b=b, b1=b1, len_unique_a=len_unique_a, pxs=pxs,\n",
    "                                 p_YgX=py_x, pys1=pys1,layer=i,num_of_bins=num_of_bins)\n",
    "\t\t\t for i in range(len(ws_iter_index))])\n",
    "    if np.mod(iter_index, interval_information_display) == 0:\n",
    "        print('Calculated The information of epoch number - {0}'.format(iter_index))\n",
    "    return params\n",
    "\n",
    "\n",
    "def extract_probs(label, x):\n",
    "    \"\"\"calculate the probabilities of the given data and labels p(x), p(y) and (y|x) \"\"\"\n",
    "    pys = np.sum(label, axis=0) / float(label.shape[0])\n",
    "    b = np.ascontiguousarray(x).view(np.dtype((np.void, x.dtype.itemsize * x.shape[1])))\n",
    "    unique_array, unique_indices, unique_inverse_x, unique_counts = \\\n",
    "    np.unique(b, return_index=True, return_inverse=True, return_counts=True)\n",
    "    unique_a = x[unique_indices]\n",
    "    b1 = np.ascontiguousarray(unique_a).view(np.dtype((np.void, unique_a.dtype.itemsize * unique_a.shape[1])))\n",
    "    pxs = unique_counts / float(np.sum(unique_counts))\n",
    "    p_y_given_x = []\n",
    "    for i in range(0, len(unique_array)):\n",
    "        indexs = unique_inverse_x == i\n",
    "        py_x_current = np.mean(label[indexs, :], axis=0)\n",
    "        p_y_given_x.append(py_x_current)\n",
    "    p_y_given_x = np.array(p_y_given_x).T\n",
    "    b_y = np.ascontiguousarray(label).view(np.dtype((np.void, label.dtype.itemsize * label.shape[1])))\n",
    "    unique_array_y, unique_indices_y, unique_inverse_y, unique_counts_y = \\\n",
    "    np.unique(b_y, return_index=True, return_inverse=True, return_counts=True)\n",
    "    pys1 = unique_counts_y / float(np.sum(unique_counts_y))\n",
    "    return pys, pys1, p_y_given_x, b1, b, unique_a, unique_inverse_x, unique_inverse_y, pxs\n",
    "\n",
    "def get_information(ws, x, label, num_of_bins, interval_information_display, \\\n",
    "                    model, layerSize, py_hats=0,multiple_bins=True,kde=False,\n",
    "                    per_layer_bins=False, lower=False, maxentropy=True):\n",
    "    \"\"\"Calculate the information for the network for all the epochs and all the layers\"\"\"\n",
    "    pys, pys1, p_y_given_x, b1, b, unique_a, unique_inverse_x, unique_inverse_y, pxs = extract_probs(label, x)\n",
    "    np.set_printoptions(edgeitems=30)\n",
    "    if per_layer_bins:\n",
    "        multiple_bins=per_layer_bins\n",
    "    print('Start calculating the information...')\n",
    "\n",
    "    multiple_bins=False\n",
    "    bins = np.linspace(-1, 1, num_of_bins)\n",
    "    label = np.array(label).astype(np.float)\n",
    "    params = np.array(Parallel(n_jobs=NUM_CORES\n",
    "        )(delayed(calc_information_for_epoch)\n",
    "        (i, interval_information_display, multiple_bins, ws[i], bins, unique_inverse_x, unique_inverse_y,\n",
    "         label,\n",
    "         b, b1, len(unique_a), pys,\n",
    "         pxs, p_y_given_x, pys1, None, x.shape[1], layerSize,num_of_bins=num_of_bins)\n",
    "        for i in range(len(ws))))\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_information(np.array(tr.hidden_activations), mutual_inf.X, mutual_inf.y_flat, 30, 1000, None, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
