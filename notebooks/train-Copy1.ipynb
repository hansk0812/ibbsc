{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "import pickle\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "import data_utils\n",
    "from random import seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f91d318e950>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1234)\n",
    "seed(1234)\n",
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "# dataset path\n",
    "data_path = \"./data/var_u.mat\" # Orig IB data\n",
    "\n",
    "# Run on GPU if possible\n",
    "cuda = torch.cuda.is_available() \n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "print(\"Using \"+ str(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"loss_function\" : nn.BCEWithLogitsLoss(),\n",
    "    \"batch_size\" : 256,\n",
    "    \"epochs\" : 200,\n",
    "    \"lr\" : 0.0004, \n",
    "    \"layer_sizes\" : [12,10,7,5,4,3,2]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "X_train, X_test, y_train, y_test = data_utils.load_data(data_path)\n",
    "\n",
    "# Prepare data for pytorch\n",
    "train_loader = data_utils.create_dataloader(X_train, y_train, len(X_train))\n",
    "test_loader = data_utils.create_dataloader(X_test, y_test, len(X_test))\n",
    "\n",
    "full_X, full_y = np.concatenate((X_train, X_test)), np.concatenate((y_train, y_test))\n",
    "eval_loader = data_utils.create_dataloader(full_X, full_y, len(full_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What do collect during training.\n",
    "# Gradients of weights for all layers: something like ib_model.h3.weight.grad\n",
    "# Weights for each layer: ib_model.h3.weight\n",
    "# Activity: Save it in the forward pass and keep track of batch things.\n",
    "\n",
    "# Compute L2 norm of weights\n",
    "# Mean of gradients\n",
    "# Std of gradients \n",
    "# Activity <- needed for MI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IBNet(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(IBNet, self).__init__()\n",
    "        torch.manual_seed(1234)\n",
    "        # See https://pytorch.org/docs/stable/nn.html\n",
    "        self.linears = nn.ModuleList()\n",
    "        \n",
    "        self.num_layers = len(cfg[\"layer_sizes\"])\n",
    "        self.inp_size = cfg[\"layer_sizes\"][0]\n",
    "        \n",
    "        h_in = self.inp_size\n",
    "        for h_out in cfg[\"layer_sizes\"][1:]:\n",
    "            self.linears.append(nn.Linear(h_in, h_out))\n",
    "            h_in = h_out\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        activations = [] #TODO: Could be nicer\n",
    "        for idx in range(self.num_layers-1):\n",
    "            x = self.linears[idx](x)\n",
    "            x = torch.tanh(x)\n",
    "            if idx == self.num_layers-1:\n",
    "                x = self.linears[-1](x)\n",
    "            if not self.training: #Internal flag in model\n",
    "                activations.append(x)\n",
    "         \n",
    "        return x, activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ib_model = IBNet(config).to(device)\n",
    "optimizer = optim.Adam(ib_model.parameters(), lr=config[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, cfg, model, optimizer):\n",
    "        self.opt = optimizer\n",
    "        self.loss_function = cfg[\"loss_function\"]\n",
    "        self.batch_size = cfg[\"batch_size\"]\n",
    "        self.epochs = cfg[\"epochs\"]\n",
    "        self.model = model\n",
    "        self.hidden_activations = [] # index 1: epoch num, index2 : layer_num\n",
    "        self.weights = dict()\n",
    "        self.ws_grads = dict()\n",
    "        \n",
    "        \n",
    "    def _get_activity(self, eval_loader):\n",
    "        \"\"\"\n",
    "        After each epoch save the activation of each hidden layer\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        eval_loss = 0\n",
    "        with torch.no_grad(): # Speeds up very little by turning autograd engine off.\n",
    "            for i, (data, label) in enumerate(eval_loader):# No need to loop.\n",
    "                data, label= data.to(device), label.to(device)\n",
    "                yhat, activations = self.model(data)\n",
    "                eval_loss += self.loss_function(yhat, label).item()\n",
    "                \n",
    "        eval_loss /= len(eval_loader.dataset)\n",
    "        print('Evaluation set loss: {:.7f}'.format(eval_loss))\n",
    "        return activations\n",
    "    \n",
    "    \n",
    "    def train(self, train_loader, test_loader, eval_loader):\n",
    "        for epoch in range(1, self.epochs+1):\n",
    "            ### START TRAIN ###\n",
    "            self.model.train()\n",
    "            train_loss = 0\n",
    "            for idx, (train_data, label) in enumerate(train_loader): \n",
    "                train_data, label = train_data.to(device), label.to(device)\n",
    "                \n",
    "                yhat, _ = self.model(train_data)\n",
    "                loss = self.loss_function(yhat, label)\n",
    "                \n",
    "                self.opt.zero_grad()\n",
    "                loss.backward()\n",
    "                train_loss += loss.item()\n",
    "                self.opt.step()\n",
    "                \n",
    "            avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "            print('Epoch: {} Average loss: {:.7f}'.format(epoch, avg_train_loss))\n",
    "            ### STOP TRAIN ###\n",
    "                \n",
    "            ### START VALIDATION ###\n",
    "            self.model.eval()\n",
    "            valid_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for i, (val_data, label) in enumerate(test_loader):\n",
    "                    val_data, label = val_data.to(device), label.to(device)\n",
    "                    yhat, activations = self.model(val_data)\n",
    "                    valid_loss += self.loss_function(yhat, label).item()\n",
    "\n",
    "            valid_loss /= len(test_loader.dataset)\n",
    "            print('Validation set loss: {:.7f}'.format(valid_loss))   \n",
    "            ### END VALIDATION ###\n",
    "            \n",
    "            ### SAVE ACTIVATION ON FULL DATA ###\n",
    "            self.hidden_activations.append(self._get_activity(eval_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = Trainer(config, ib_model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3277"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Average loss: 0.0002129\n",
      "Validation set loss: 0.0008523\n",
      "Evaluation set loss: 0.0001704\n",
      "Epoch: 2 Average loss: 0.0002129\n",
      "Validation set loss: 0.0008523\n",
      "Evaluation set loss: 0.0001703\n",
      "Epoch: 3 Average loss: 0.0002129\n",
      "Validation set loss: 0.0008522\n",
      "Evaluation set loss: 0.0001703\n",
      "Epoch: 4 Average loss: 0.0002129\n",
      "Validation set loss: 0.0008521\n",
      "Evaluation set loss: 0.0001703\n",
      "Epoch: 5 Average loss: 0.0002128\n",
      "Validation set loss: 0.0008520\n",
      "Evaluation set loss: 0.0001703\n",
      "Epoch: 6 Average loss: 0.0002128\n",
      "Validation set loss: 0.0008519\n",
      "Evaluation set loss: 0.0001703\n",
      "Epoch: 7 Average loss: 0.0002128\n",
      "Validation set loss: 0.0008519\n",
      "Evaluation set loss: 0.0001703\n",
      "Epoch: 8 Average loss: 0.0002128\n",
      "Validation set loss: 0.0008518\n",
      "Evaluation set loss: 0.0001702\n",
      "Epoch: 9 Average loss: 0.0002128\n",
      "Validation set loss: 0.0008517\n",
      "Evaluation set loss: 0.0001702\n",
      "Epoch: 10 Average loss: 0.0002127\n",
      "Validation set loss: 0.0008516\n",
      "Evaluation set loss: 0.0001702\n",
      "Epoch: 11 Average loss: 0.0002127\n",
      "Validation set loss: 0.0008515\n",
      "Evaluation set loss: 0.0001702\n",
      "Epoch: 12 Average loss: 0.0002127\n",
      "Validation set loss: 0.0008514\n",
      "Evaluation set loss: 0.0001702\n",
      "Epoch: 13 Average loss: 0.0002127\n",
      "Validation set loss: 0.0008514\n",
      "Evaluation set loss: 0.0001702\n",
      "Epoch: 14 Average loss: 0.0002127\n",
      "Validation set loss: 0.0008513\n",
      "Evaluation set loss: 0.0001701\n",
      "Epoch: 15 Average loss: 0.0002126\n",
      "Validation set loss: 0.0008512\n",
      "Evaluation set loss: 0.0001701\n",
      "Epoch: 16 Average loss: 0.0002126\n",
      "Validation set loss: 0.0008511\n",
      "Evaluation set loss: 0.0001701\n",
      "Epoch: 17 Average loss: 0.0002126\n",
      "Validation set loss: 0.0008510\n",
      "Evaluation set loss: 0.0001701\n",
      "Epoch: 18 Average loss: 0.0002126\n",
      "Validation set loss: 0.0008509\n",
      "Evaluation set loss: 0.0001701\n",
      "Epoch: 19 Average loss: 0.0002125\n",
      "Validation set loss: 0.0008509\n",
      "Evaluation set loss: 0.0001701\n",
      "Epoch: 20 Average loss: 0.0002125\n",
      "Validation set loss: 0.0008508\n",
      "Evaluation set loss: 0.0001700\n",
      "Epoch: 21 Average loss: 0.0002125\n",
      "Validation set loss: 0.0008507\n",
      "Evaluation set loss: 0.0001700\n",
      "Epoch: 22 Average loss: 0.0002125\n",
      "Validation set loss: 0.0008506\n",
      "Evaluation set loss: 0.0001700\n",
      "Epoch: 23 Average loss: 0.0002125\n",
      "Validation set loss: 0.0008505\n",
      "Evaluation set loss: 0.0001700\n",
      "Epoch: 24 Average loss: 0.0002124\n",
      "Validation set loss: 0.0008504\n",
      "Evaluation set loss: 0.0001700\n",
      "Epoch: 25 Average loss: 0.0002124\n",
      "Validation set loss: 0.0008503\n",
      "Evaluation set loss: 0.0001699\n",
      "Epoch: 26 Average loss: 0.0002124\n",
      "Validation set loss: 0.0008502\n",
      "Evaluation set loss: 0.0001699\n",
      "Epoch: 27 Average loss: 0.0002124\n",
      "Validation set loss: 0.0008502\n",
      "Evaluation set loss: 0.0001699\n",
      "Epoch: 28 Average loss: 0.0002123\n",
      "Validation set loss: 0.0008501\n",
      "Evaluation set loss: 0.0001699\n",
      "Epoch: 29 Average loss: 0.0002123\n",
      "Validation set loss: 0.0008500\n",
      "Evaluation set loss: 0.0001699\n",
      "Epoch: 30 Average loss: 0.0002123\n",
      "Validation set loss: 0.0008499\n",
      "Evaluation set loss: 0.0001699\n",
      "Epoch: 31 Average loss: 0.0002123\n",
      "Validation set loss: 0.0008498\n",
      "Evaluation set loss: 0.0001698\n",
      "Epoch: 32 Average loss: 0.0002123\n",
      "Validation set loss: 0.0008497\n",
      "Evaluation set loss: 0.0001698\n",
      "Epoch: 33 Average loss: 0.0002122\n",
      "Validation set loss: 0.0008496\n",
      "Evaluation set loss: 0.0001698\n",
      "Epoch: 34 Average loss: 0.0002122\n",
      "Validation set loss: 0.0008495\n",
      "Evaluation set loss: 0.0001698\n",
      "Epoch: 35 Average loss: 0.0002122\n",
      "Validation set loss: 0.0008494\n",
      "Evaluation set loss: 0.0001698\n",
      "Epoch: 36 Average loss: 0.0002122\n",
      "Validation set loss: 0.0008493\n",
      "Evaluation set loss: 0.0001697\n",
      "Epoch: 37 Average loss: 0.0002121\n",
      "Validation set loss: 0.0008492\n",
      "Evaluation set loss: 0.0001697\n",
      "Epoch: 38 Average loss: 0.0002121\n",
      "Validation set loss: 0.0008491\n",
      "Evaluation set loss: 0.0001697\n",
      "Epoch: 39 Average loss: 0.0002121\n",
      "Validation set loss: 0.0008491\n",
      "Evaluation set loss: 0.0001697\n",
      "Epoch: 40 Average loss: 0.0002121\n",
      "Validation set loss: 0.0008490\n",
      "Evaluation set loss: 0.0001697\n",
      "Epoch: 41 Average loss: 0.0002120\n",
      "Validation set loss: 0.0008489\n",
      "Evaluation set loss: 0.0001696\n",
      "Epoch: 42 Average loss: 0.0002120\n",
      "Validation set loss: 0.0008488\n",
      "Evaluation set loss: 0.0001696\n",
      "Epoch: 43 Average loss: 0.0002120\n",
      "Validation set loss: 0.0008487\n",
      "Evaluation set loss: 0.0001696\n",
      "Epoch: 44 Average loss: 0.0002120\n",
      "Validation set loss: 0.0008486\n",
      "Evaluation set loss: 0.0001696\n",
      "Epoch: 45 Average loss: 0.0002119\n",
      "Validation set loss: 0.0008485\n",
      "Evaluation set loss: 0.0001696\n",
      "Epoch: 46 Average loss: 0.0002119\n",
      "Validation set loss: 0.0008483\n",
      "Evaluation set loss: 0.0001695\n",
      "Epoch: 47 Average loss: 0.0002119\n",
      "Validation set loss: 0.0008482\n",
      "Evaluation set loss: 0.0001695\n",
      "Epoch: 48 Average loss: 0.0002118\n",
      "Validation set loss: 0.0008481\n",
      "Evaluation set loss: 0.0001695\n",
      "Epoch: 49 Average loss: 0.0002118\n",
      "Validation set loss: 0.0008480\n",
      "Evaluation set loss: 0.0001695\n",
      "Epoch: 50 Average loss: 0.0002118\n",
      "Validation set loss: 0.0008479\n",
      "Evaluation set loss: 0.0001694\n",
      "Epoch: 51 Average loss: 0.0002118\n",
      "Validation set loss: 0.0008478\n",
      "Evaluation set loss: 0.0001694\n",
      "Epoch: 52 Average loss: 0.0002117\n",
      "Validation set loss: 0.0008477\n",
      "Evaluation set loss: 0.0001694\n",
      "Epoch: 53 Average loss: 0.0002117\n",
      "Validation set loss: 0.0008476\n",
      "Evaluation set loss: 0.0001694\n",
      "Epoch: 54 Average loss: 0.0002117\n",
      "Validation set loss: 0.0008475\n",
      "Evaluation set loss: 0.0001694\n",
      "Epoch: 55 Average loss: 0.0002116\n",
      "Validation set loss: 0.0008474\n",
      "Evaluation set loss: 0.0001693\n",
      "Epoch: 56 Average loss: 0.0002116\n",
      "Validation set loss: 0.0008472\n",
      "Evaluation set loss: 0.0001693\n",
      "Epoch: 57 Average loss: 0.0002116\n",
      "Validation set loss: 0.0008471\n",
      "Evaluation set loss: 0.0001693\n",
      "Epoch: 58 Average loss: 0.0002116\n",
      "Validation set loss: 0.0008470\n",
      "Evaluation set loss: 0.0001693\n",
      "Epoch: 59 Average loss: 0.0002115\n",
      "Validation set loss: 0.0008469\n",
      "Evaluation set loss: 0.0001692\n",
      "Epoch: 60 Average loss: 0.0002115\n",
      "Validation set loss: 0.0008468\n",
      "Evaluation set loss: 0.0001692\n",
      "Epoch: 61 Average loss: 0.0002115\n",
      "Validation set loss: 0.0008466\n",
      "Evaluation set loss: 0.0001692\n",
      "Epoch: 62 Average loss: 0.0002114\n",
      "Validation set loss: 0.0008465\n",
      "Evaluation set loss: 0.0001692\n",
      "Epoch: 63 Average loss: 0.0002114\n",
      "Validation set loss: 0.0008464\n",
      "Evaluation set loss: 0.0001691\n",
      "Epoch: 64 Average loss: 0.0002114\n",
      "Validation set loss: 0.0008463\n",
      "Evaluation set loss: 0.0001691\n",
      "Epoch: 65 Average loss: 0.0002113\n",
      "Validation set loss: 0.0008461\n",
      "Evaluation set loss: 0.0001691\n",
      "Epoch: 66 Average loss: 0.0002113\n",
      "Validation set loss: 0.0008460\n",
      "Evaluation set loss: 0.0001690\n",
      "Epoch: 67 Average loss: 0.0002113\n",
      "Validation set loss: 0.0008459\n",
      "Evaluation set loss: 0.0001690\n",
      "Epoch: 68 Average loss: 0.0002112\n",
      "Validation set loss: 0.0008457\n",
      "Evaluation set loss: 0.0001690\n",
      "Epoch: 69 Average loss: 0.0002112\n",
      "Validation set loss: 0.0008456\n",
      "Evaluation set loss: 0.0001690\n",
      "Epoch: 70 Average loss: 0.0002111\n",
      "Validation set loss: 0.0008455\n",
      "Evaluation set loss: 0.0001689\n",
      "Epoch: 71 Average loss: 0.0002111\n",
      "Validation set loss: 0.0008453\n",
      "Evaluation set loss: 0.0001689\n",
      "Epoch: 72 Average loss: 0.0002111\n",
      "Validation set loss: 0.0008452\n",
      "Evaluation set loss: 0.0001689\n",
      "Epoch: 73 Average loss: 0.0002110\n",
      "Validation set loss: 0.0008450\n",
      "Evaluation set loss: 0.0001688\n",
      "Epoch: 74 Average loss: 0.0002110\n",
      "Validation set loss: 0.0008449\n",
      "Evaluation set loss: 0.0001688\n",
      "Epoch: 75 Average loss: 0.0002110\n",
      "Validation set loss: 0.0008447\n",
      "Evaluation set loss: 0.0001688\n",
      "Epoch: 76 Average loss: 0.0002109\n",
      "Validation set loss: 0.0008446\n",
      "Evaluation set loss: 0.0001687\n",
      "Epoch: 77 Average loss: 0.0002109\n",
      "Validation set loss: 0.0008444\n",
      "Evaluation set loss: 0.0001687\n",
      "Epoch: 78 Average loss: 0.0002108\n",
      "Validation set loss: 0.0008443\n",
      "Evaluation set loss: 0.0001687\n",
      "Epoch: 79 Average loss: 0.0002108\n",
      "Validation set loss: 0.0008441\n",
      "Evaluation set loss: 0.0001686\n",
      "Epoch: 80 Average loss: 0.0002108\n",
      "Validation set loss: 0.0008439\n",
      "Evaluation set loss: 0.0001686\n",
      "Epoch: 81 Average loss: 0.0002107\n",
      "Validation set loss: 0.0008438\n",
      "Evaluation set loss: 0.0001686\n",
      "Epoch: 82 Average loss: 0.0002107\n",
      "Validation set loss: 0.0008436\n",
      "Evaluation set loss: 0.0001685\n",
      "Epoch: 83 Average loss: 0.0002106\n",
      "Validation set loss: 0.0008434\n",
      "Evaluation set loss: 0.0001685\n",
      "Epoch: 84 Average loss: 0.0002106\n",
      "Validation set loss: 0.0008433\n",
      "Evaluation set loss: 0.0001685\n",
      "Epoch: 85 Average loss: 0.0002105\n",
      "Validation set loss: 0.0008431\n",
      "Evaluation set loss: 0.0001684\n",
      "Epoch: 86 Average loss: 0.0002105\n",
      "Validation set loss: 0.0008429\n",
      "Evaluation set loss: 0.0001684\n",
      "Epoch: 87 Average loss: 0.0002104\n",
      "Validation set loss: 0.0008427\n",
      "Evaluation set loss: 0.0001684\n",
      "Epoch: 88 Average loss: 0.0002104\n",
      "Validation set loss: 0.0008425\n",
      "Evaluation set loss: 0.0001683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 89 Average loss: 0.0002103\n",
      "Validation set loss: 0.0008424\n",
      "Evaluation set loss: 0.0001683\n",
      "Epoch: 90 Average loss: 0.0002103\n",
      "Validation set loss: 0.0008422\n",
      "Evaluation set loss: 0.0001682\n",
      "Epoch: 91 Average loss: 0.0002102\n",
      "Validation set loss: 0.0008420\n",
      "Evaluation set loss: 0.0001682\n",
      "Epoch: 92 Average loss: 0.0002102\n",
      "Validation set loss: 0.0008418\n",
      "Evaluation set loss: 0.0001682\n",
      "Epoch: 93 Average loss: 0.0002101\n",
      "Validation set loss: 0.0008416\n",
      "Evaluation set loss: 0.0001681\n",
      "Epoch: 94 Average loss: 0.0002101\n",
      "Validation set loss: 0.0008414\n",
      "Evaluation set loss: 0.0001681\n",
      "Epoch: 95 Average loss: 0.0002100\n",
      "Validation set loss: 0.0008412\n",
      "Evaluation set loss: 0.0001680\n",
      "Epoch: 96 Average loss: 0.0002100\n",
      "Validation set loss: 0.0008410\n",
      "Evaluation set loss: 0.0001680\n",
      "Epoch: 97 Average loss: 0.0002099\n",
      "Validation set loss: 0.0008407\n",
      "Evaluation set loss: 0.0001679\n",
      "Epoch: 98 Average loss: 0.0002099\n",
      "Validation set loss: 0.0008405\n",
      "Evaluation set loss: 0.0001679\n",
      "Epoch: 99 Average loss: 0.0002098\n",
      "Validation set loss: 0.0008403\n",
      "Evaluation set loss: 0.0001679\n",
      "Epoch: 100 Average loss: 0.0002098\n",
      "Validation set loss: 0.0008401\n",
      "Evaluation set loss: 0.0001678\n",
      "Epoch: 101 Average loss: 0.0002097\n",
      "Validation set loss: 0.0008399\n",
      "Evaluation set loss: 0.0001678\n",
      "Epoch: 102 Average loss: 0.0002096\n",
      "Validation set loss: 0.0008396\n",
      "Evaluation set loss: 0.0001677\n",
      "Epoch: 103 Average loss: 0.0002096\n",
      "Validation set loss: 0.0008394\n",
      "Evaluation set loss: 0.0001677\n",
      "Epoch: 104 Average loss: 0.0002095\n",
      "Validation set loss: 0.0008391\n",
      "Evaluation set loss: 0.0001676\n",
      "Epoch: 105 Average loss: 0.0002095\n",
      "Validation set loss: 0.0008389\n",
      "Evaluation set loss: 0.0001676\n",
      "Epoch: 106 Average loss: 0.0002094\n",
      "Validation set loss: 0.0008387\n",
      "Evaluation set loss: 0.0001675\n",
      "Epoch: 107 Average loss: 0.0002093\n",
      "Validation set loss: 0.0008384\n",
      "Evaluation set loss: 0.0001675\n",
      "Epoch: 108 Average loss: 0.0002093\n",
      "Validation set loss: 0.0008382\n",
      "Evaluation set loss: 0.0001674\n",
      "Epoch: 109 Average loss: 0.0002092\n",
      "Validation set loss: 0.0008379\n",
      "Evaluation set loss: 0.0001674\n",
      "Epoch: 110 Average loss: 0.0002091\n",
      "Validation set loss: 0.0008376\n",
      "Evaluation set loss: 0.0001673\n",
      "Epoch: 111 Average loss: 0.0002091\n",
      "Validation set loss: 0.0008374\n",
      "Evaluation set loss: 0.0001672\n",
      "Epoch: 112 Average loss: 0.0002090\n",
      "Validation set loss: 0.0008371\n",
      "Evaluation set loss: 0.0001672\n",
      "Epoch: 113 Average loss: 0.0002089\n",
      "Validation set loss: 0.0008368\n",
      "Evaluation set loss: 0.0001671\n",
      "Epoch: 114 Average loss: 0.0002088\n",
      "Validation set loss: 0.0008365\n",
      "Evaluation set loss: 0.0001671\n",
      "Epoch: 115 Average loss: 0.0002088\n",
      "Validation set loss: 0.0008362\n",
      "Evaluation set loss: 0.0001670\n",
      "Epoch: 116 Average loss: 0.0002087\n",
      "Validation set loss: 0.0008360\n",
      "Evaluation set loss: 0.0001670\n",
      "Epoch: 117 Average loss: 0.0002086\n",
      "Validation set loss: 0.0008357\n",
      "Evaluation set loss: 0.0001669\n",
      "Epoch: 118 Average loss: 0.0002085\n",
      "Validation set loss: 0.0008354\n",
      "Evaluation set loss: 0.0001668\n",
      "Epoch: 119 Average loss: 0.0002085\n",
      "Validation set loss: 0.0008351\n",
      "Evaluation set loss: 0.0001668\n",
      "Epoch: 120 Average loss: 0.0002084\n",
      "Validation set loss: 0.0008348\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-46475ecff455>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-c5850ae1ad53>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_loader, test_loader, eval_loader)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;31m### SAVE ACTIVATION ON FULL DATA ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_activations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_activity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-c5850ae1ad53>\u001b[0m in \u001b[0;36m_get_activity\u001b[0;34m(self, eval_loader)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0meval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Speeds up very little by turning autograd engine off.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m# No need to loop.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0myhat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tr.train(train_loader, test_loader, eval_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tr.hidden_activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.2035,  0.2650, -0.3538,  0.2026, -0.3642, -0.0774,  0.1970],\n",
      "        [-0.2346,  0.3401,  0.2999,  0.3037,  0.2033, -0.0376,  0.2969],\n",
      "        [ 0.0186, -0.2844, -0.2035,  0.3039, -0.3554, -0.0389,  0.1780],\n",
      "        [-0.0967, -0.0714, -0.1287,  0.0062,  0.0489, -0.2657,  0.3678],\n",
      "        [-0.0112,  0.0172,  0.1437, -0.2345,  0.0525,  0.2868, -0.0176]],\n",
      "       requires_grad=True)\n",
      "(5, 7)\n",
      "Parameter containing:\n",
      "tensor([[-0.2039,  0.2654, -0.3542,  0.2022, -0.3638, -0.0778,  0.1974],\n",
      "        [-0.2342,  0.3397,  0.3003,  0.3041,  0.2029, -0.0372,  0.2965],\n",
      "        [ 0.0190, -0.2848, -0.2031,  0.3043, -0.3558, -0.0385,  0.1776],\n",
      "        [-0.0971, -0.0710, -0.1291,  0.0058,  0.0493, -0.2661,  0.3682],\n",
      "        [-0.0116,  0.0176,  0.1433, -0.2349,  0.0529,  0.2864, -0.0172]],\n",
      "       requires_grad=True)\n",
      "(5, 7)\n",
      "Parameter containing:\n",
      "tensor([[-0.2043,  0.2658, -0.3545,  0.2019, -0.3634, -0.0782,  0.1978],\n",
      "        [-0.2338,  0.3393,  0.3006,  0.3044,  0.2025, -0.0368,  0.2961],\n",
      "        [ 0.0194, -0.2852, -0.2027,  0.3045, -0.3562, -0.0381,  0.1772],\n",
      "        [-0.0975, -0.0706, -0.1294,  0.0055,  0.0497, -0.2665,  0.3686],\n",
      "        [-0.0120,  0.0180,  0.1429, -0.2352,  0.0533,  0.2860, -0.0168]],\n",
      "       requires_grad=True)\n",
      "(5, 7)\n",
      "Parameter containing:\n",
      "tensor([[-0.2046,  0.2662, -0.3549,  0.2018, -0.3630, -0.0786,  0.1982],\n",
      "        [-0.2334,  0.3389,  0.3010,  0.3045,  0.2022, -0.0364,  0.2957],\n",
      "        [ 0.0198, -0.2856, -0.2024,  0.3047, -0.3566, -0.0377,  0.1768],\n",
      "        [-0.0978, -0.0702, -0.1298,  0.0054,  0.0501, -0.2669,  0.3689],\n",
      "        [-0.0123,  0.0184,  0.1425, -0.2354,  0.0537,  0.2856, -0.0164]],\n",
      "       requires_grad=True)\n",
      "(5, 7)\n",
      "Parameter containing:\n",
      "tensor([[-0.2050,  0.2666, -0.3553,  0.2015, -0.3626, -0.0790,  0.1986],\n",
      "        [-0.2330,  0.3385,  0.3014,  0.3048,  0.2018, -0.0360,  0.2953],\n",
      "        [ 0.0201, -0.2860, -0.2020,  0.3049, -0.3570, -0.0373,  0.1765],\n",
      "        [-0.0982, -0.0698, -0.1302,  0.0052,  0.0505, -0.2673,  0.3693],\n",
      "        [-0.0127,  0.0188,  0.1421, -0.2357,  0.0541,  0.2852, -0.0160]],\n",
      "       requires_grad=True)\n",
      "(5, 7)\n",
      "Parameter containing:\n",
      "tensor([[-0.2054,  0.2670, -0.3557,  0.2013, -0.3623, -0.0794,  0.1990],\n",
      "        [-0.2327,  0.3381,  0.3018,  0.3050,  0.2014, -0.0356,  0.2949],\n",
      "        [ 0.0205, -0.2864, -0.2016,  0.3052, -0.3574, -0.0369,  0.1761],\n",
      "        [-0.0986, -0.0694, -0.1306,  0.0049,  0.0509, -0.2677,  0.3697],\n",
      "        [-0.0131,  0.0192,  0.1417, -0.2360,  0.0545,  0.2848, -0.0156]],\n",
      "       requires_grad=True)\n",
      "(5, 7)\n",
      "Parameter containing:\n",
      "tensor([[-0.2058,  0.2674, -0.3560,  0.2010, -0.3619, -0.0798,  0.1994],\n",
      "        [-0.2323,  0.3377,  0.3022,  0.3053,  0.2010, -0.0352,  0.2945],\n",
      "        [ 0.0209, -0.2868, -0.2012,  0.3054, -0.3578, -0.0365,  0.1757],\n",
      "        [-0.0989, -0.0690, -0.1310,  0.0046,  0.0513, -0.2681,  0.3701],\n",
      "        [-0.0134,  0.0196,  0.1414, -0.2362,  0.0549,  0.2844, -0.0153]],\n",
      "       requires_grad=True)\n",
      "(5, 7)\n",
      "Parameter containing:\n",
      "tensor([[-0.2062,  0.2678, -0.3564,  0.2007, -0.3615, -0.0802,  0.1998],\n",
      "        [-0.2319,  0.3374,  0.3026,  0.3056,  0.2006, -0.0348,  0.2941],\n",
      "        [ 0.0213, -0.2872, -0.2008,  0.3057, -0.3582, -0.0361,  0.1753],\n",
      "        [-0.0993, -0.0686, -0.1313,  0.0043,  0.0517, -0.2685,  0.3705],\n",
      "        [-0.0138,  0.0200,  0.1410, -0.2366,  0.0553,  0.2840, -0.0149]],\n",
      "       requires_grad=True)\n",
      "(5, 7)\n",
      "Parameter containing:\n",
      "tensor([[-0.2065,  0.2682, -0.3568,  0.2004, -0.3611, -0.0806,  0.2002],\n",
      "        [-0.2315,  0.3370,  0.3029,  0.3059,  0.2002, -0.0344,  0.2937],\n",
      "        [ 0.0217, -0.2876, -0.2004,  0.3061, -0.3586, -0.0357,  0.1748],\n",
      "        [-0.0997, -0.0682, -0.1317,  0.0040,  0.0521, -0.2689,  0.3709],\n",
      "        [-0.0142,  0.0204,  0.1406, -0.2369,  0.0557,  0.2836, -0.0145]],\n",
      "       requires_grad=True)\n",
      "(5, 7)\n",
      "Parameter containing:\n",
      "tensor([[-0.2069,  0.2686, -0.3572,  0.2001, -0.3607, -0.0810,  0.2006],\n",
      "        [-0.2311,  0.3366,  0.3033,  0.3062,  0.1998, -0.0340,  0.2933],\n",
      "        [ 0.0220, -0.2879, -0.2000,  0.3064, -0.3590, -0.0353,  0.1745],\n",
      "        [-0.1001, -0.0678, -0.1321,  0.0037,  0.0525, -0.2693,  0.3713],\n",
      "        [-0.0146,  0.0208,  0.1402, -0.2372,  0.0561,  0.2832, -0.0141]],\n",
      "       requires_grad=True)\n",
      "(5, 7)\n",
      "Parameter containing:\n",
      "tensor([[-0.2073,  0.2690, -0.3576,  0.1997, -0.3603, -0.0814,  0.2010],\n",
      "        [-0.2308,  0.3362,  0.3037,  0.3066,  0.1994, -0.0336,  0.2929],\n",
      "        [ 0.0224, -0.2883, -0.1996,  0.3067, -0.3594, -0.0349,  0.1741],\n",
      "        [-0.1005, -0.0675, -0.1325,  0.0033,  0.0529, -0.2697,  0.3717],\n",
      "        [-0.0150,  0.0211,  0.1399, -0.2376,  0.0564,  0.2828, -0.0137]],\n",
      "       requires_grad=True)\n",
      "(5, 7)\n",
      "Parameter containing:\n",
      "tensor([[-0.2077,  0.2694, -0.3580,  0.1994, -0.3599, -0.0818,  0.2014],\n",
      "        [-0.2304,  0.3358,  0.3041,  0.3069,  0.1990, -0.0332,  0.2925],\n",
      "        [ 0.0228, -0.2887, -0.1992,  0.3071, -0.3598, -0.0345,  0.1737],\n",
      "        [-0.1009, -0.0671, -0.1329,  0.0030,  0.0533, -0.2701,  0.3721],\n",
      "        [-0.0154,  0.0215,  0.1395, -0.2379,  0.0568,  0.2824, -0.0133]],\n",
      "       requires_grad=True)\n",
      "(5, 7)\n",
      "Parameter containing:\n",
      "tensor([[-0.2081,  0.2697, -0.3583,  0.1990, -0.3595, -0.0822,  0.2018],\n",
      "        [-0.2300,  0.3354,  0.3045,  0.3072,  0.1986, -0.0328,  0.2922],\n",
      "        [ 0.0232, -0.2891, -0.1988,  0.3074, -0.3602, -0.0341,  0.1733],\n",
      "        [-0.1012, -0.0667, -0.1333,  0.0027,  0.0536, -0.2704,  0.3725],\n",
      "        [-0.0157,  0.0219,  0.1391, -0.2383,  0.0572,  0.2821, -0.0129]],\n",
      "       requires_grad=True)\n",
      "(5, 7)\n",
      "Epoch: 1 Average loss: 0.0029051\n",
      "Validation set loss: -0.0024672\n"
     ]
    }
   ],
   "source": [
    "ib_model = IBNet().to(device)\n",
    "optimizer = optim.Adam(ib_model.parameters(), lr=0.0004)\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "epochs = 1\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "        ### Start train pass ###\n",
    "        ib_model.train()\n",
    "        train_loss = 0\n",
    "        for idx, (data, label) in enumerate(train_loader):\n",
    "            data = data.to(device)\n",
    "            label = label.long().to(device)\n",
    "\n",
    "            yhat = ib_model(data)\n",
    "            loss = loss_function(torch.log(yhat), label) #NLLLoss needs log probas.\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Get gradients for each layer\n",
    "            print(ib_model.h3.weight) # for 3'rd hidden layer\n",
    "            print(ib_model.h3.weight.detach().numpy().shape)\n",
    "        avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "        print('Epoch: {} Average loss: {:.7f}'.format(epoch, avg_train_loss))\n",
    "        ### End train pass ###\n",
    "\n",
    "        ### Start evaluate on validation set ###\n",
    "        ib_model.eval()\n",
    "        valid_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for i, (data, label) in enumerate(test_loader):\n",
    "                data = data.to(device)\n",
    "                label = label.long().to(device)\n",
    "                \n",
    "                yhat = ib_model(data)\n",
    "                valid_loss += loss_function(yhat, label).item()\n",
    "        \n",
    "        valid_loss /= len(test_loader.dataset)\n",
    "        print('Validation set loss: {:.7f}'.format(valid_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
